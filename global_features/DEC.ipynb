{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b1df0c6",
   "metadata": {},
   "source": [
    "version DEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b759f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),       \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, embedding_dim) \n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, input_dim) \n",
    "            #sigmoid ?\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded, encoded\n",
    "\n",
    "#les dim à revoir !!\n",
    "F_global_dim = 2048\n",
    "F_important_dim = 512\n",
    "F_local_dim = 1024\n",
    "\n",
    "input_dim = F_global_dim + F_important_dim + F_local_dim # Dimension entrée concaténée\n",
    "emb_dim = 128  # Dimension de l'espace latent pour le clustering \n",
    "\n",
    "autoencoder = Autoencoder(input_dim, emb_dim)\n",
    "\n",
    "criterion_pretrain = nn.MSELoss() #tester d'autres maybe\n",
    "optimizer_pretrain = optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "data = # le tensor de nos features \n",
    "\n",
    "dataset = TensorDataset(data)\n",
    "dataloader_pretrain = DataLoader(dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for batch_idx, (data,) in enumerate(dataloader_pretrain):\n",
    "        optimizer_pretrain.zero_grad()\n",
    "        reconstructed_data, _ = autoencoder(data)\n",
    "        loss = criterion_pretrain(reconstructed_data, data)\n",
    "        loss.backward()\n",
    "        optimizer_pretrain.step()\n",
    "        total_loss += loss.item() * data.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader_pretrain.dataset)\n",
    "    print(f\"Epoch Pré-entraînement [{epoch+1}/{epochs}], Perte: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b921dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "class DEC(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, num_clusters, pretrained_ae=None):\n",
    "        super(DEC, self).__init__()\n",
    "        self.encoder = pretrained_ae.encoder if pretrained_ae else Autoencoder(input_dim, embedding_dim).encoder\n",
    "        \n",
    "        self.num_clusters = num_clusters\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.cluster_layer = nn.Parameter(torch.Tensor(num_clusters, embedding_dim))\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        embeddings = self.encoder(x)\n",
    "        #on utilise la distrib student t distrib \n",
    "        q = 1.0 / (1.0 + torch.sum(torch.pow(embeddings.unsqueeze(1) - self.cluster_layer, 2), 2) / 1.0)\n",
    "        q = q.pow((1.0 + 1.0) / 2.0)\n",
    "        q = (q.T / torch.sum(q, 1)).T \n",
    "        \n",
    "        return embeddings, q\n",
    "\n",
    "def target_distribution(q):\n",
    "    weight = q**2 / torch.sum(q, 0)\n",
    "    return (weight.T / torch.sum(weight, 1)).T\n",
    "\n",
    "k = 10 # jsp lol faut tester\n",
    "\n",
    "dec_model = DEC(input_dim, emb_dim, k, pretrained_ae=autoencoder)\n",
    "\n",
    "autoencoder.eval() \n",
    "all_embeddings_pretrain = []\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data,) in enumerate(dataloader_pretrain):\n",
    "        _, encoded = autoencoder(data)\n",
    "        all_embeddings_pretrain.append(encoded.cpu().numpy())\n",
    "all_embeddings_pretrain = np.concatenate(all_embeddings_pretrain, axis=0)\n",
    "\n",
    "kmeans = KMeans(n_clusters=k, n_init=20, random_state=0)\n",
    "kmeans.fit(all_embeddings_pretrain)\n",
    "initial_cluster_centroids = torch.tensor(kmeans.cluster_centers_, dtype=torch.float)\n",
    "\n",
    "dec_model.cluster_layer.data = initial_cluster_centroids\n",
    "\n",
    "optimizer_dec = optim.Adam(dec_model.parameters(), lr=0.001)\n",
    "dataloader_dec = DataLoader(dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "num_epochs_dec = 100 # Plus d'epochs pour l'affinement\n",
    "update_interval = 140 \n",
    "previous_cluster_assignments = None\n",
    "\n",
    "for epoch in range(num_epochs_dec):\n",
    "    total_loss_dec = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    if epoch % 1 == 0: \n",
    "        dec_model.eval() \n",
    "        all_q = []\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data,) in enumerate(dataloader_dec):\n",
    "                _, q_batch = dec_model(data)\n",
    "                all_q.append(q_batch.cpu())\n",
    "        all_q = torch.cat(all_q, dim=0)\n",
    "        p_target = target_distribution(all_q).to(data.device) \n",
    "        current_cluster_assignments = torch.argmax(all_q, dim=1).numpy()\n",
    "        if previous_cluster_assignments is not None:\n",
    "            n_changes = np.sum(current_cluster_assignments != previous_cluster_assignments)\n",
    "            print(f\"Epoch {epoch}: {n_changes} assignations ont changé.\")\n",
    "            if n_changes < 0.001 * len(dataset): \n",
    "                break\n",
    "        previous_cluster_assignments = current_cluster_assignments\n",
    "        dec_model.train() \n",
    "\n",
    "    for batch_idx, (data,) in enumerate(dataloader_dec):\n",
    "        optimizer_dec.zero_grad()\n",
    "        \n",
    "        _, q_batch = dec_model(data)\n",
    "\n",
    "        if len(dataloader_dec.dataset) < 10000: \n",
    "            loss_kl = F.kl_div(q_batch.log(), p_target[batch_idx*data.size(0):(batch_idx+1)*data.size(0)], reduction='batchmean')\n",
    "        else:\n",
    "            loss_kl = F.kl_div(q_batch.log(), target_distribution(q_batch), reduction='batchmean')\n",
    "\n",
    "\n",
    "        loss_kl.backward()\n",
    "        optimizer_dec.step()\n",
    "        total_loss_dec += loss_kl.item() * data.size(0)\n",
    "        num_batches += 1\n",
    "        \n",
    "    avg_loss_dec = total_loss_dec / len(dataloader_dec.dataset)\n",
    "    print(f\"Epoch DEC [{epoch+1}/{num_epochs_dec}], Perte KL: {avg_loss_dec:.4f}\")\n",
    "\n",
    "dec_model.eval()\n",
    "final_embeddings_dec = []\n",
    "final_cluster_assignments = []\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data,) in enumerate(dataloader_dec):\n",
    "        embeddings, q_batch = dec_model(data)\n",
    "        final_embeddings_dec.append(embeddings.cpu().numpy())\n",
    "        final_cluster_assignments.append(torch.argmax(q_batch, dim=1).cpu().numpy())\n",
    "\n",
    "final_embeddings_dec = np.concatenate(final_embeddings_dec, axis=0)\n",
    "final_cluster_assignments = np.concatenate(final_cluster_assignments, axis=0)\n",
    "\n",
    "print(f\"Exemple {final_cluster_assignments[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c5618e",
   "metadata": {},
   "source": [
    "VERsion GMM : c'est plus simple mais en fait c'est peut etre moins adapté "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed103ae0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
