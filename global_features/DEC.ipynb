{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b1df0c6",
   "metadata": {},
   "source": [
    "version DEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "90be34af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "bfa578c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),       \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, embedding_dim) \n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, input_dim) \n",
    "            #sigmoid ?\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded, encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "45d910a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embeddings_np = np.load(\"../embeddings/merged_embeddings.npy\")\n",
    "data = torch.tensor(embeddings_np, dtype=torch.float)\n",
    "dataset = TensorDataset(data)\n",
    "dataloader = DataLoader(dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "train_embeddings, test_embeddings = train_test_split(embeddings_np, test_size=0.2, random_state=42)\n",
    "train_data = torch.tensor(train_embeddings, dtype=torch.float)\n",
    "test_data = torch.tensor(test_embeddings, dtype=torch.float)\n",
    "\n",
    "train_dataset = TensorDataset(train_data)\n",
    "test_dataset = TensorDataset(test_data)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "input_dim = embeddings_np.shape[1]\n",
    "embedding_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "377aba2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "autoencoder = Autoencoder(input_dim, embedding_dim)\n",
    "\n",
    "autoencoder.to(device)\n",
    "criterion_pretrain = nn.MSELoss() #tester d'autres maybe\n",
    "optimizer_pretrain = optim.AdamW(autoencoder.parameters(), lr=0.001, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2b759f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Loss: 196.0834\n",
      "Epoch [51/1000], Loss: 11.8243\n",
      "Epoch [101/1000], Loss: 6.2941\n",
      "Epoch [151/1000], Loss: 4.8958\n",
      "Epoch [201/1000], Loss: 4.1334\n",
      "Epoch [251/1000], Loss: 3.7617\n",
      "Epoch [301/1000], Loss: 3.1265\n",
      "Epoch [351/1000], Loss: 2.7008\n",
      "Epoch [401/1000], Loss: 2.8219\n",
      "Epoch [451/1000], Loss: 2.5849\n",
      "Epoch [501/1000], Loss: 2.2049\n",
      "Epoch [551/1000], Loss: 2.2421\n",
      "Epoch [601/1000], Loss: 2.1664\n",
      "Epoch [651/1000], Loss: 2.2481\n",
      "Epoch [701/1000], Loss: 2.2795\n",
      "Epoch [751/1000], Loss: 2.3155\n",
      "Epoch [801/1000], Loss: 2.0140\n",
      "Epoch [851/1000], Loss: 1.6907\n",
      "Epoch [901/1000], Loss: 1.8684\n",
      "Epoch [951/1000], Loss: 2.0264\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for batch_idx, (data,) in enumerate(loop):\n",
    "        data = data.to(device)  # <-- la ligne cruciale !\n",
    "\n",
    "        optimizer_pretrain.zero_grad()\n",
    "        reconstructed_data, _ = autoencoder(data)\n",
    "        loss = criterion_pretrain(reconstructed_data, data)\n",
    "        loss.backward()\n",
    "        optimizer_pretrain.step()\n",
    "\n",
    "        total_loss += loss.item() * data.size(0)\n",
    "        loop.set_postfix(batch_loss=loss.item())\n",
    "\n",
    "    avg_loss = total_loss / len(train_dataloader.dataset)\n",
    "    if (epoch) % 50 == 0:\n",
    "        tqdm.write(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "649994a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss (MSE): 2.9350\n"
     ]
    }
   ],
   "source": [
    "autoencoder.eval()\n",
    "with torch.no_grad():\n",
    "    test_data = test_data.to(device)\n",
    "    reconstructed_test, encoded_test = autoencoder(test_data)\n",
    "    test_loss = criterion_pretrain(reconstructed_test, test_data)\n",
    "print(f\"Test Loss (MSE): {test_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "221b32a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined embeddings shape: (8739, 256)\n"
     ]
    }
   ],
   "source": [
    "# Use the encoder to generate embeddings for the entire dataset\n",
    "embeddings_np = np.load(\"../embeddings/merged_embeddings.npy\")\n",
    "data = torch.tensor(embeddings_np, dtype=torch.float)\n",
    "\n",
    "autoencoder.eval()\n",
    "with torch.no_grad():\n",
    "    data = data.to(device)\n",
    "    _, combined_embeddings = autoencoder(data)\n",
    "    combined_embeddings_np = combined_embeddings.cpu().numpy()\n",
    "print(f\"Combined embeddings shape: {combined_embeddings_np.shape}\")\n",
    "# Save the embeddings to a .npy file\n",
    "np.save(\"../embeddings/combined_embeddings.npy\", combined_embeddings_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0b921dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PresiJean\\.conda\\envs\\vae-gpu\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch DEC [1/100], Perte KL: 0.1685\n",
      "Epoch 1: 3459 assignations ont changé.\n",
      "Epoch DEC [2/100], Perte KL: 0.0062\n",
      "Epoch 2: 265 assignations ont changé.\n",
      "Epoch DEC [3/100], Perte KL: 0.0035\n",
      "Epoch 3: 99 assignations ont changé.\n",
      "Epoch DEC [4/100], Perte KL: 0.0019\n",
      "Epoch 4: 29 assignations ont changé.\n",
      "Epoch DEC [5/100], Perte KL: 0.0011\n",
      "Epoch 5: 8 assignations ont changé.\n",
      "Exemple [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "class DEC(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, num_clusters, pretrained_ae=None):\n",
    "        super(DEC, self).__init__()\n",
    "        self.encoder = pretrained_ae.encoder if pretrained_ae else Autoencoder(input_dim, embedding_dim).encoder\n",
    "        \n",
    "        self.num_clusters = num_clusters\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.cluster_layer = nn.Parameter(torch.Tensor(num_clusters, embedding_dim))\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        embeddings = self.encoder(x)\n",
    "        self.cluster_layer.data = self.cluster_layer.data.to(embeddings.device)\n",
    "        #on utilise la distrib student t distrib \n",
    "        diff = embeddings.unsqueeze(1) - self.cluster_layer  # (B, K, D)\n",
    "        dist_sq = torch.sum(diff ** 2, dim=2)                # (B, K)\n",
    "        q = 1.0 / (1.0 + dist_sq)\n",
    "        q = q ** ((1 + 1) / 2.0)\n",
    "        q = q / torch.sum(q, dim=1, keepdim=True)\n",
    "        \n",
    "        return embeddings, q\n",
    "\n",
    "def target_distribution(q):\n",
    "    weight = q**2 / torch.sum(q, 0)\n",
    "    return (weight.T / torch.sum(weight, 1)).T\n",
    "\n",
    "k = 6 # jsp lol faut tester\n",
    "\n",
    "dec_model = DEC(input_dim, embedding_dim, k, pretrained_ae=autoencoder).to(device)\n",
    "\n",
    "autoencoder.eval() \n",
    "all_embeddings_pretrain = []\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data,) in enumerate(dataloader):\n",
    "        data = data[0].to(device)\n",
    "        _, encoded = autoencoder(data)\n",
    "        if encoded.dim() == 1:\n",
    "            encoded = encoded.unsqueeze(0)\n",
    "        all_embeddings_pretrain.append(encoded.cpu().numpy())\n",
    "        \n",
    "        \n",
    "all_embeddings_pretrain = np.concatenate(all_embeddings_pretrain, axis=0)\n",
    "\n",
    "kmeans = KMeans(n_clusters=k, n_init=20, random_state=0)\n",
    "kmeans.fit(all_embeddings_pretrain)\n",
    "initial_cluster_centroids = torch.tensor(kmeans.cluster_centers_, dtype=torch.float)\n",
    "initial_cluster_centroids = initial_cluster_centroids.to(device)\n",
    "\n",
    "dec_model.cluster_layer.data = initial_cluster_centroids\n",
    "\n",
    "\n",
    "optimizer_dec = optim.Adam(dec_model.parameters(), lr=0.001)\n",
    "dataloader_dec = DataLoader(dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "num_epochs_dec = 100 # Plus d'epochs pour l'affinement\n",
    "update_interval = 140 \n",
    "previous_cluster_assignments = None\n",
    "\n",
    "for epoch in range(num_epochs_dec):\n",
    "    total_loss_dec = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    if epoch % 1 == 0: \n",
    "        dec_model.eval() \n",
    "        all_q = []\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data,) in enumerate(dataloader_dec):\n",
    "                data = data.to(device)\n",
    "                _, q_batch = dec_model(data)\n",
    "                all_q.append(q_batch.cpu())\n",
    "        all_q = torch.cat(all_q, dim=0)\n",
    "        p_target = target_distribution(all_q).to(data.device) \n",
    "        current_cluster_assignments = torch.argmax(all_q, dim=1).numpy()\n",
    "        if previous_cluster_assignments is not None:\n",
    "            n_changes = np.sum(current_cluster_assignments != previous_cluster_assignments)\n",
    "            print(f\"Epoch {epoch}: {n_changes} assignations ont changé.\")\n",
    "            if n_changes < 0.001 * len(dataset): \n",
    "                break\n",
    "        previous_cluster_assignments = current_cluster_assignments\n",
    "        dec_model.train() \n",
    "\n",
    "    for batch_idx, (data,) in enumerate(dataloader_dec):\n",
    "        data = data.to(device)\n",
    "        optimizer_dec.zero_grad()\n",
    "        \n",
    "        _, q_batch = dec_model(data)\n",
    "\n",
    "        if len(dataloader_dec.dataset) < 10000: \n",
    "            loss_kl = F.kl_div(q_batch.log(), p_target[batch_idx*data.size(0):(batch_idx+1)*data.size(0)], reduction='batchmean')\n",
    "        else:\n",
    "            loss_kl = F.kl_div(q_batch.log(), target_distribution(q_batch), reduction='batchmean')\n",
    "\n",
    "\n",
    "        loss_kl.backward()\n",
    "        optimizer_dec.step()\n",
    "        total_loss_dec += loss_kl.item() * data.size(0)\n",
    "        num_batches += 1\n",
    "        \n",
    "    avg_loss_dec = total_loss_dec / len(dataloader_dec.dataset)\n",
    "    print(f\"Epoch DEC [{epoch+1}/{num_epochs_dec}], Perte KL: {avg_loss_dec:.4f}\")\n",
    "\n",
    "dec_model.eval()\n",
    "final_embeddings_dec = []\n",
    "final_cluster_assignments = []\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data,) in enumerate(dataloader_dec):\n",
    "        data = data.to(device)\n",
    "        embeddings, q_batch = dec_model(data)\n",
    "        final_embeddings_dec.append(embeddings.cpu().numpy())\n",
    "        final_cluster_assignments.append(torch.argmax(q_batch, dim=1).cpu().numpy())\n",
    "\n",
    "final_embeddings_dec = np.concatenate(final_embeddings_dec, axis=0)\n",
    "final_cluster_assignments = np.concatenate(final_cluster_assignments, axis=0)\n",
    "\n",
    "print(f\"Exemple {final_cluster_assignments[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c5618e",
   "metadata": {},
   "source": [
    "VERsion GMM : c'est plus simple mais en fait c'est peut etre moins adapté "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed103ae0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc2ffd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vae-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
